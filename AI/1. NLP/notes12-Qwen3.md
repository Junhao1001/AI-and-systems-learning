# Qwen3

## 0. æœ¬åœ°ç¯å¢ƒé…ç½®

### git clone transformers

```
git clone https://github.com/huggingface/transformers.git
```

### PyCharmåˆ›å»ºé¡¹ç›®

- æ‰“å¼€pycharmï¼Œopenä¹‹å‰cloneçš„`transformer`ç›®å½•
- åˆ›å»ºè™šæ‹Ÿç¯å¢ƒï¼š
  - å³ä¸‹è§’æ‰“å¼€Python interpreter
  - ç‚¹å‡» **Add Interpreter**
  - é€‰æ‹© **Virtualenv Environment**
  - é€‰æ‹©ï¼šBase interpreterï¼š`Python 3.10`

- å®‰è£…æˆåŠŸåï¼š
  - å³ä¸‹è§’æ˜¾ç¤ºï¼š`Python 3.10 (venv)`
  - `transformers/venv/` ç›®å½•å‡ºç°
  - PyCharm Terminal è‡ªåŠ¨æ¿€æ´» venv

### ç”¨ PyCharm Terminal å®‰è£…ä¾èµ–

- å®‰è£… PyTorchï¼ˆCPU ç‰ˆæœ¬å³å¯ï¼‰

```
pip install torch --index-url https://download.pytorch.org/whl/cpu
```

- Editable æ¨¡å¼å®‰è£… Transformersï¼ˆğŸ”¥å…³é”®ï¼‰

```
pip install -e .
```

è¿™ä¸€æ­¥éå¸¸é‡è¦ï¼Œå®ƒæ„å‘³ç€ï¼š`import transformers` ç”¨çš„æ˜¯ **ä½ æ­£åœ¨ç¼–è¾‘çš„æºç **

- å®‰è£… Qwen3 ç›¸å…³ä¾èµ–

```
pip install accelerate sentencepiece safetensors einops
```

### éªŒè¯ transformers å¯è¿è¡Œ

- æ ¹ç›®å½•åˆ›å»º`run_test_qwen.py`æ–‡ä»¶

```python
from transformers import AutoTokenizer, AutoModelForCausalLM

print("transformers import ok")

tokenizer = AutoTokenizer.from_pretrained(
    "Qwen/Qwen3-0.6B",
    trust_remote_code=True
)
model = AutoModelForCausalLM.from_pretrained(
    "Qwen/Qwen3-0.6B",
    trust_remote_code=True
)

print("Qwen3 loaded")

```

- è¿è¡Œåæœ‰å¯¹åº”è¾“å‡º



## 0.5 Some Concepts

### HF (Hugging Face)

**Hugging Face Transformers æä¾›çš„ä¸€æ•´å¥—ã€Œæ¨¡å‹å·¥ç¨‹è§„èŒƒã€**

HFå®šä¹‰äº†ï¼š

- æ¨¡å‹åº”è¯¥é•¿ä»€ä¹ˆæ ·
- forward åº”è¯¥æ¥æ”¶ä»€ä¹ˆå‚æ•°
- generate æ€ä¹ˆç»Ÿä¸€è°ƒç”¨ä¸åŒæ¨¡å‹
- è¾“å‡ºæ ¼å¼ï¼ˆdataclassï¼‰

### helper function

**helperr function: ä¸æ˜¯æ¨¡å‹æœ¬ä½“ï¼Œä½†è®©æ¨¡å‹æ›´å¥½å†™ / æ›´å¥½ç”¨çš„å°å‡½æ•°**

å…¶ä¸æ˜¯ä¸€ä¸ªç»“æ„ï¼Œè€Œæ˜¯ä¸€äº›**å·¥å…·**

æ¨¡å‹ä¸­å¸¸ç”¨çš„**helperå‡½æ•°**ï¼š

- æ„é€  attention mask
- å¤„ç† KV cache
- reshape / expand tensor
- å¤„ç† rotary embedding çš„ index

### Pytorchå±‚ ï¼ˆnn.Moduleï¼‰?

pytorchå±‚æ˜¯**ä¸€ä¸ªå¸¦å‚æ•°+å¯å‘å‰è®¡ç®—çš„å‡½æ•°**

æ•°å­¦è§’åº¦ï¼š

```
y = f(x; Î¸)
```

åœ¨Qwen3 ä¸­ï¼Œæœ‰å¦‚ä¸‹pytorchå±‚ï¼š

| PyTorch å±‚          | æ•°å­¦æ„ä¹‰           |
| ------------------- | ------------------ |
| `Qwen3Attention`    | Attention æ˜ å°„     |
| `Qwen3MLP`          | FFN                |
| `RMSNorm`           | å½’ä¸€åŒ–             |
| `Qwen3DecoderLayer` | ä¸€æ•´å±‚ Transformer |

### ä¸å¸¦ä»»åŠ¡å¤´çš„çº¯è¯­è¨€æ¨¡å‹

å…¶ä½œç”¨æ˜¯ï¼š**æŠŠ token åºåˆ— â†’ æ˜ å°„ä¸ºâ€œä¸Šä¸‹æ–‡è¯­ä¹‰è¡¨ç¤ºâ€**

```
input_ids
  â†“
embedding
  â†“
Transformer layers
  â†“
hidden_states

```

- å®ƒ**ä¸ä¼šè®¡ç®—è¯è¡¨æ¦‚ç‡å’Œé€‰ä¸‹ä¸€ä¸ªtoken**
- å®ƒ**åªè¾“å‡ºè¡¨ç¤º**

### lm_head

ç”¨äºå°†**è¡¨ç¤ºè½¬æ¢ä¸ºè¯è¡¨æ¦‚ç‡**

ä½œç”¨å¦‚ä¸‹ï¼š

```
hidden_states
  â†“
lm_head
  â†“
logits
  â†“
softmax
  â†“
token probability
```

lm_head **ä¸æ˜¯ Transformer çš„ä¸€éƒ¨åˆ†**

å®ƒæ˜¯ä¸€ä¸ª **ä»»åŠ¡å¤´ï¼ˆtask headï¼‰**

åŒä¸€ä¸ª backboneï¼š

- å¯ä»¥æ¥ LM head
- ä¹Ÿå¯ä»¥æ¥ classification head

### Others

- **past_key_values**: Attention ä¸­ç¼“å­˜çš„ Key / Value
- **hidden_states**: æ¯ä¸ª token å¯¹åº”çš„â€œè¯­ä¹‰å‘é‡è¡¨ç¤ºâ€



## 1. æ•´ä½“ç»“æ„åˆ†æ

`modeling_qwen3.py`å¤§è‡´å¯ä»¥åˆ†ä¸º7ä¸ªæ¨¡å—ï¼š

```markdown
1. imports + å·¥å…·å‡½æ•°
2. è¾…åŠ©å°æ¨¡å—ï¼ˆNorm / MLP / Rotaryï¼‰
3. Attention å®ç°
4. Decoder Layer
5. Backbone Modelï¼ˆQwen3Modelï¼‰
6. Task Headï¼ˆQwen3ForCausalLMï¼‰
7. HF æ³¨å†Œ & æ–‡æ¡£ç›¸å…³ä»£ç 
```

### 1.1 imports + é€šç”¨å·¥å…·

- å¸¸ç”¨å·¥å…·ï¼Œå¦‚ï¼š`torch`,`nn`,`F`
- HF çš„ï¼š
  - `PreTrainedModel`
  - `BaseModelOutputWithPast`
  - `CausalLMOutputWithPast`
- ä¸€äº› helper å‡½æ•°ï¼ˆmask / cacheï¼‰

### 1.2 åŸºç¡€ç»„ä»¶(Building Blocks)

- å°†è®ºæ–‡é‡Œçš„æ•°å­¦æ¨¡å—å˜æˆPyTorchå±‚
- å®šä¹‰äº†**Transformerçš„ä¸€äº›å°æ¨¡å—**ï¼Œå¯èƒ½ä¼šè¢«Attention/ DecoderLayer è°ƒç”¨
  - `RMSNorm`
  - `Qwen3MLP`
  - Rotary Embedding ç›¸å…³å‡½æ•°

### 1.3 Qwen3Attention

è¯¥æ¨¡å—è´Ÿè´£ï¼š

- Q / K / V æŠ•å½±
- RoPEï¼ˆæ—‹è½¬ä½ç½®ç¼–ç ï¼‰
- GQA / MQA
- KV Cacheï¼ˆpast_key_valuesï¼‰
- causal mask

### 1.4 Qwen3DecoderLayer

å®ç°**ä¸€å±‚æ ‡å‡†decoder block:**

å…¶åŸºæœ¬ç»“æ„ä¸ºï¼š

```
x
 â”œâ”€ RMSNorm
 â”œâ”€ Attention
 â”œâ”€ Residual
 â”œâ”€ RMSNorm
 â”œâ”€ MLP
 â””â”€ Residual
```

- éœ€è¦å…³æ³¨å„å±‚çš„é¡ºåº
- residual å¦‚ä½•è¿›è¡Œå¢åŠ 
- å¦‚ä½•æ’å…¥attention / nlp

### 1.5 Qwen3Model (Backbone)

ä¾æ¬¡å¤„ç†å¦‚ä¸‹äº‹æƒ…ï¼š

1. embedding input_ids
2. ä¾æ¬¡è·‘ N å±‚ `Qwen3DecoderLayer`
3. ç®¡ç†ï¼š
   - attention_mask
   - position_ids
   - past_key_values
4. æœ€ååšä¸€ä¸ª norm

`Qwen3Model` çº¦ä¸º**â€œä¸å¸¦ä»»åŠ¡å¤´çš„çº¯è¯­è¨€æ¨¡å‹â€**ï¼Œå…¶è¾“å‡ºä¸º

- hidden_states
- past_key_values

### 1.6 Qwen3ForCausalLM

ä»£ç é‡Œè°ƒç”¨å¾—åˆ°æ˜¯ï¼š

```
AutoModelForCausalLM â†’ Qwen3ForCausalLM
```

å…¶ä¼šæ‰§è¡Œå¦‚ä¸‹å‘½ä»¤ï¼š

- è°ƒç”¨ `Qwen3Model.forward`
- æ¥ä¸€ä¸ª `lm_head`
- è®¡ç®— logits / loss

### 1.7 HF glueä»£ç 

åŒ…æ‹¬ï¼š

- `_CONFIG_FOR_DOC`
- `@add_start_docstrings`
- `register_for_auto_class`

**ä½œç”¨**ï¼š

- æ–‡æ¡£
- AutoModel è¯†åˆ«
- HuggingFace ç”Ÿæ€å…¼å®¹

### æ•´ä½“è°ƒç”¨é“¾

ä»`generate()`æ¥å£å¼€å§‹ï¼Œæ¨¡å‹çš„è°ƒç”¨é“¾ä¸ºï¼š

```scss
model.generate()
  â†“
GenerationMixin
  â†“
Qwen3ForCausalLM.forward()
  â†“
Qwen3Model.forward()
  â†“
for layer in layers:
      Qwen3DecoderLayer.forward()
          â†“
          Qwen3Attention.forward()
          Qwen3MLP.forward()
```



## 2. Qwen3ForCausal

### 2.1 åˆå§‹åŒ–

```python
self.model = Qwen3Model(config)
self.vocab_size = config.vocab_size
self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)

# Initialize weights and apply final processing
self.post_init()
```

- `self.qwen3`ï¼šDecoder-only Transformerçš„ä¸»é¢˜

- `self.lm_head`: ä»»åŠ¡å¤´ï¼Œå°†hidden_states æ˜ å°„åˆ°è¯è¡¨

- `self.post_init()`: **HuggingFace `PreTrainedModel` ç»Ÿä¸€çš„â€œæ¨¡å‹åˆå§‹åŒ–æ”¶å°¾é’©å­â€**

  - æƒé‡åˆå§‹åŒ–
  - æƒé‡tying ï¼ˆä¹‹åå†æ¥ç†è§£å«ä¹‰ï¼Œè¿™é‡Œå…ˆä¸æ·±å…¥ï¼‰
  - æ³¨å†Œ gradient checkpoint/ flash attention ç­‰åå¤„ç†é€»è¾‘

- `post_init()` å®šä¹‰åœ¨ **`PreTrainedModel`** é‡Œã€‚

  ```
  Qwen3ForCausalLM
   â””â”€â”€ Qwen3PreTrainedModel
       â””â”€â”€ PreTrainedModel
  ```

  

### 2.2 è°ƒç”¨backbone

- è°ƒç”¨model(Qwen3Model)

```python
outputs: BaseModelOutputWithPast = self.model(
            input_ids=input_ids,
            attention_mask=attention_mask,
            position_ids=position_ids,
            past_key_values=past_key_values,
            inputs_embeds=inputs_embeds,
            use_cache=use_cache,
            cache_position=cache_position,
            **kwargs,
        )
```

- è¾“å…¥ï¼š

  - `input_ids` ï¼š token id

    - è¿™ä¸€è½® forward è¦å¤„ç†çš„æ‰€æœ‰ token
    - é•¿åº¦ä¸å®šï¼Œç¬¬ä¸€è½®æ˜¯promptçš„æ‰€æœ‰token
    - åç»­å°±æ˜¯æ–°ç”Ÿæˆçš„ä¸€ä¸ªtoken

  - `attention_mask` ï¼špadding / causal mask

    - é¿å…attention çœ‹è§padding
    - 1ä¸ºå¯è§ï¼Œ0ä¸ºpadding/mask

  - `position_ids`ï¼štokençš„ä½ç½®ç¼–å·ï¼Œç”¨äº**RoPE / rotary embedding**

  - `past_key_values` â†’ KV cacheï¼Œç”¨äºåŠ é€Ÿç”Ÿæˆ

    ```python
    past_key_values = Tuple[
        layer_0(k, v),
        layer_1(k, v),
        ...
    ]
    ```

    - key: `[batch, heads, past_len, head_dim]`
    - value: åŒä¸Š

  - `input_embeds`: å¯ä»¥ç»•è¿‡embedding lookup,ä¸€èˆ¬ä¸å’Œ`input_ids`åŒæ—¶ä¸Šä¼ 

    - åœ¨å¤šæ¨¡æ€æ¨¡å‹ä¸­ï¼Œå¯èƒ½æœ‰çš„embeddingæ²¡æœ‰token id
    - Prompt tuning / Soft prompt, æå‰å¤„ç†äº†token embeddings

  - `use_cache`: boolï¼Œç”¨äºç¡®è®¤æ˜¯å¦è¿”å›`past_key_values`

    - æ¨ç†æ—¶: True    è®­ç»ƒæ—¶:  False

  - `cache_position`: qwen3ä¸­è¾ƒæ–°çš„ï¼Œ**æ˜¾ç¤ºå‘Šè¯‰æ¨¡å‹å½“å‰ token åœ¨â€œå…¨åºåˆ—ä¸­çš„ç»å¯¹ä½ç½®â€**

    - å¸¸ç”¨äºé™æ€ KV cache
    - Flashattention v2ç­‰
    - Long context

- è¾“å‡ºï¼š

  - `hidden_states` â†’ æ¯ä¸ª token çš„è¡¨ç¤º
  - `past_key_values` â†’ æ›´æ–°åçš„ KV cache

### 2.3 hidden_states åˆ° logits

- logitsï¼šæ¯ä¸ª token å¯¹è¯è¡¨çš„â€œæ‰“åˆ†â€ï¼Œè¿˜æ²¡ softmax

```python
    hidden_states = outputs.last_hidden_state
    # Only compute necessary logits, and do not upcast them to float if we are not computing the loss
    slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep
    logits = self.lm_head(hidden_states[:, slice_indices, :])
```

- è¿›è¡Œä¸€ä¸ªçº¿æ€§æ˜ å°„ï¼Œä»`hidden_dim â†’ vocab_size`

- `slice_indices`: ç®—åŠ›ä¼˜åŒ–

  - å‡è®¾è¾“å…¥é•¿åº¦ = 4096ï¼Œä½†ä½ åªæƒ³é¢„æµ‹æœ€åä¸€ä¸ª token (ä¸€èˆ¬åœ¨æ¨ç†æ—¶)
  - å¦‚æœç›´æ¥

  ```
  logits = lm_head(hidden_states)
  ä¼šå¾—åˆ°
  [batch, 4096, vocab]
  ```

  - é€šå¸¸

  ```
  slice_indices = [-1]
  ```

  - åªä¿ç•™æœ€åä¸€ä¸ªtokençš„ hidden state
  - **åœ¨è®­ç»ƒæ—¶ï¼Œä»ç„¶éœ€è¦è®¡ç®—æ‰€æœ‰çš„logitsï¼Œç”¨äºlossè®¡ç®—**

### 2.4 è®¡ç®—loss

```python
loss = None
if labels is not None:
    loss = self.loss_function(logits=logits, labels=labels, vocab_size=self.config.vocab_size, **kwargs)
```

- è®­ç»ƒé˜¶æ®µæä¾› labelsï¼š
  - logits ä¼šå’Œ labels å¯¹é½
  - è®¡ç®—æ ‡å‡† **cross-entropy loss**
- åœ¨æ¨ç†é˜¶æ®µï¼š
  - `labels=None`
  - æ‰€ä»¥ä¸ä¼šè®¡ç®— loss

### 2.5 è¾“å‡ºç»“æ„

```python
return CausalLMOutputWithPast(
    loss=loss,
    logits=logits,
    past_key_values=outputs.past_key_values,
    hidden_states=outputs.hidden_states,
    attentions=outputs.attentions,
)
```

- HF ä½¿ç”¨ç»Ÿä¸€ dataclass æ¥å°è£…è¾“å‡º

- åŒ…å«ï¼š

  - `logits`: é¢„æµ‹æ¦‚ç‡å‰çš„å‘é‡

  - `past_key_values`: KV cache

  - `hidden_states`: ä¸­é—´è¡¨ç¤º

  - `loss` : è®­ç»ƒæŸå¤±ï¼ˆoptionalï¼‰

  - `attention`: æ¯ä¸€å±‚çš„attention map, é»˜è®¤æ˜¯none

    ```python
    attentions[layer] =
        [batch_size, num_heads, tgt_len, src_len]
    ```

    